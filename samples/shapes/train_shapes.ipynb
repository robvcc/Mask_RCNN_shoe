{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfigurations:\nBACKBONE                       resnet101\nBACKBONE_STRIDES               [4, 8, 16, 32, 64]\nBATCH_SIZE                     1\nBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\nCOMPUTE_BACKBONE_SHAPE         None\nDETECTION_MAX_INSTANCES        100\nDETECTION_MIN_CONFIDENCE       0.7\nDETECTION_NMS_THRESHOLD        0.3\nFPN_CLASSIF_FC_LAYERS_SIZE     1024\nGPU_COUNT                      1\nGRADIENT_CLIP_NORM             5.0\nIMAGES_PER_GPU                 1\nIMAGE_CHANNEL_COUNT            3\nIMAGE_MAX_DIM                  128\nIMAGE_META_SIZE                16\nIMAGE_MIN_DIM                  128\nIMAGE_MIN_SCALE                0\nIMAGE_RESIZE_MODE              square\nIMAGE_SHAPE                    [128 128   3]\nLEARNING_MOMENTUM              0.9\nLEARNING_RATE                  0.001\nLOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\nMASK_POOL_SIZE                 14\nMASK_SHAPE                     [28, 28]\nMAX_GT_INSTANCES               100\nMEAN_PIXEL                     [123.7 116.8 103.9]\nMINI_MASK_SHAPE                (56, 56)\nNAME                           shapes\nNUM_CLASSES                    4\nPOOL_SIZE                      7\nPOST_NMS_ROIS_INFERENCE        1000\nPOST_NMS_ROIS_TRAINING         2000\nPRE_NMS_LIMIT                  6000\nROI_POSITIVE_RATIO             0.33\nRPN_ANCHOR_RATIOS              [0.5, 1, 2]\nRPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\nRPN_ANCHOR_STRIDE              1\nRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\nRPN_NMS_THRESHOLD              0.7\nRPN_TRAIN_ANCHORS_PER_IMAGE    256\nSTEPS_PER_EPOCH                10\nTOP_DOWN_PYRAMID_SIZE          256\nTRAIN_BN                       False\nTRAIN_ROIS_PER_IMAGE           32\nUSE_MINI_MASK                  True\nUSE_RPN_ROIS                   True\nVALIDATION_STEPS               5\nWEIGHT_DECAY                   0.0001\n\n\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 10\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ljt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1154: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ljt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ljt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStarting at epoch 0. LR=0.001\n\nCheckpoint Path: /home/ljt/Mask_RCNN/logs/shapes20181019T1543/mask_rcnn_shapes_{epoch:04d}.h5\nSelecting layers to train\nfpn_c5p5               (Conv2D)\nfpn_c4p4               (Conv2D)\nfpn_c3p3               (Conv2D)\nfpn_c2p2               (Conv2D)\nfpn_p5                 (Conv2D)\nfpn_p2                 (Conv2D)\nfpn_p3                 (Conv2D)\nfpn_p4                 (Conv2D)\nIn model:  rpn_model\n    rpn_conv_shared        (Conv2D)\n    rpn_class_raw          (Conv2D)\n    rpn_bbox_pred          (Conv2D)\nmrcnn_mask_conv1       (TimeDistributed)\nmrcnn_mask_bn1         (TimeDistributed)\nmrcnn_mask_conv2       (TimeDistributed)\nmrcnn_mask_bn2         (TimeDistributed)\nmrcnn_class_conv1      (TimeDistributed)\nmrcnn_class_bn1        (TimeDistributed)\nmrcnn_mask_conv3       (TimeDistributed)\nmrcnn_mask_bn3         (TimeDistributed)\nmrcnn_class_conv2      (TimeDistributed)\nmrcnn_class_bn2        (TimeDistributed)\nmrcnn_mask_conv4       (TimeDistributed)\nmrcnn_mask_bn4         (TimeDistributed)\nmrcnn_bbox_fc          (TimeDistributed)\nmrcnn_mask_deconv      (TimeDistributed)\nmrcnn_class_logits     (TimeDistributed)\nmrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 1/10 [==>...........................] - ETA: 74s - loss: 6.0646 - rpn_class_loss: 0.1298 - rpn_bbox_loss: 2.1203 - mrcnn_class_loss: 1.2418 - mrcnn_bbox_loss: 1.6164 - mrcnn_mask_loss: 0.9563"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2/10 [=====>........................] - ETA: 39s - loss: 4.9134 - rpn_class_loss: 0.0787 - rpn_bbox_loss: 1.6744 - mrcnn_class_loss: 1.0754 - mrcnn_bbox_loss: 1.2859 - mrcnn_mask_loss: 0.7991"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3/10 [========>.....................] - ETA: 26s - loss: 5.3498 - rpn_class_loss: 0.1120 - rpn_bbox_loss: 2.0795 - mrcnn_class_loss: 0.9686 - mrcnn_bbox_loss: 1.3355 - mrcnn_mask_loss: 0.8542"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4/10 [===========>..................] - ETA: 19s - loss: 4.8281 - rpn_class_loss: 0.1047 - rpn_bbox_loss: 1.7732 - mrcnn_class_loss: 0.8560 - mrcnn_bbox_loss: 1.2605 - mrcnn_mask_loss: 0.8337"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5/10 [==============>...............] - ETA: 14s - loss: 4.7925 - rpn_class_loss: 0.1067 - rpn_bbox_loss: 1.7985 - mrcnn_class_loss: 0.8817 - mrcnn_bbox_loss: 1.2031 - mrcnn_mask_loss: 0.8026"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6/10 [=================>............] - ETA: 10s - loss: 4.6302 - rpn_class_loss: 0.0952 - rpn_bbox_loss: 1.8608 - mrcnn_class_loss: 0.7954 - mrcnn_bbox_loss: 1.1431 - mrcnn_mask_loss: 0.7357"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7/10 [====================>.........] - ETA: 7s - loss: 4.3590 - rpn_class_loss: 0.0849 - rpn_bbox_loss: 1.6942 - mrcnn_class_loss: 0.7467 - mrcnn_bbox_loss: 1.0998 - mrcnn_mask_loss: 0.7335 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8/10 [=======================>......] - ETA: 4s - loss: 4.2463 - rpn_class_loss: 0.0872 - rpn_bbox_loss: 1.5840 - mrcnn_class_loss: 0.7959 - mrcnn_bbox_loss: 1.0766 - mrcnn_mask_loss: 0.7027"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9/10 [==========================>...] - ETA: 2s - loss: 4.0763 - rpn_class_loss: 0.0885 - rpn_bbox_loss: 1.4732 - mrcnn_class_loss: 0.8088 - mrcnn_bbox_loss: 1.0337 - mrcnn_mask_loss: 0.6721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py:2142: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 46s - loss: 3.9926 - rpn_class_loss: 0.0902 - rpn_bbox_loss: 1.4135 - mrcnn_class_loss: 0.7905 - mrcnn_bbox_loss: 1.0175 - mrcnn_mask_loss: 0.6810 - val_loss: 3.2339 - val_rpn_class_loss: 0.0580 - val_rpn_bbox_loss: 1.0594 - val_mrcnn_class_loss: 0.4120 - val_mrcnn_bbox_loss: 1.0336 - val_mrcnn_mask_loss: 0.6710"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Path:  /deepmatter/mask_rcnn/logs/shapes2017102802/mask_rcnn_{epoch:04d}.h5\n",
      "Starting at epoch 0. LR=0.0002\n",
      "\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 86s - loss: 11.4006 - rpn_class_loss: 0.0184 - rpn_bbox_loss: 0.8409 - mrcnn_class_loss: 0.1576 - mrcnn_bbox_loss: 0.0902 - mrcnn_mask_loss: 0.1977 - val_loss: 11.4376 - val_rpn_class_loss: 0.0220 - val_rpn_bbox_loss: 1.0068 - val_mrcnn_class_loss: 0.1172 - val_mrcnn_bbox_loss: 0.0683 - val_mrcnn_mask_loss: 0.1278\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/ljt/Mask_RCNN/logs/shapes20181019T1543/mask_rcnn_shapes_0000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-starting from epoch 0\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   67.00000  max:  222.00000  uint8\nimage_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int64\ngt_class_id              shape: (1,)                  min:    1.00000  max:    1.00000  int32\ngt_bbox                  shape: (1, 4)                min:   16.00000  max:   78.00000  int32\ngt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\nimage                    shape: (128, 128, 3)         min:    9.00000  max:  237.00000  uint8\nmolded_images            shape: (1, 128, 128, 3)      min: -114.70000  max:  120.20000  float64\nimage_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int64\nanchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n*** No instances to display *** \n\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.8416666681567827\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
